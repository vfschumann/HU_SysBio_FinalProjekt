{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "542a3f68-f019-41b6-a2c9-1070807ebda9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Implementation of a Autoencoder (AE)\n",
    "#### based on PyTorch tutorial (https://pytorch.org/tutorials/beginner/basics/intro.html)\n",
    "## Simple AE with one layer into latent space 748 -> 10\n",
    "##### I hope it also does it backwards (at least it works, but other implementations explicitly specify the backward layer and this one does not (in NeuralNetwork_AE)).\n",
    "#### Build for MINST datasets\n",
    "\n",
    "###### ***For sc-RNAseq:*** fix enable cuda/GPU (only needed if CPU is to slow), try different optimiser, check for loss function, trying learning rates, nn.ReLU() function may not be the best, checking for overfitting by plotting loss of model and training data, Batch size has to be optimised: https://arxiv.org/abs/1609.04836 , https://arxiv.org/abs/1703.04933 . Maybe we should try a program like https://opendatascience.com/optimizing-pytorch-performance-batch-size-with-pytorch-profiler/ for that on a later stage for performance (not sure if the data will be to large in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c170cac6-0e7e-4c2c-98ed-c26c1c7b2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544b516d-aa5f-4481-b7a0-d73f2f9f8657",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd006b3-bf5d-4077-be2d-789f3cd15016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   ### needed if Nvidia GPU is available and wanted to use the GPU\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39e5f45-c408-44e4-97bb-fdb582f3b34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork_AE(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Building the neural network structure ####\n",
    "\n",
    "class NeuralNetwork_AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork_AE, self).__init__()\n",
    "        self.flatten = nn.Flatten() # transform tensor\n",
    "        self.linear_relu_stack = nn.Sequential( # sequential operation of the following code:\n",
    "            nn.Linear(28*28, 512),  #### data into the neural network layer (28*28 pixel -> 512 nodes)\n",
    "            nn.ReLU(),             #### update the values output of previos layer by the function: ReLU(x) = (x)^+ = max(0,x) \n",
    "            nn.Linear(512, 10),  #### layer into the hidden layer / latent space (512 -> 10 nodes / latent vector)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # exicute the nn defined in __init__(self)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x) \n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork_AE() #.to(device) #### this is needed for cuda\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c03c64-62cb-4f52-a5a0-4ac0231c598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training loop #### \n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()    ### set the optimizer to none gradient\n",
    "        loss.backward()          ### compute the loss\n",
    "        optimizer.step()         ### Performs a single optimization step (parameter update)\n",
    "        if batch % 100 == 0:     ### after 100 batches print the result\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn): ### compute the error and accuracy of the epoch\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5912a895-8d3f-4fc1-a5ff-4b0edbcab99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4 ### how fast the parameter gets updated\n",
    "epochs = 5           ### how many times it starts over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69467eb8-c0f8-4ecd-a3ac-8cc4908e1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function:\n",
    "    # nn.MSELoss (Mean Square Error) for regression tasks\n",
    "    # nn.NLLLoss (Negative Log Likelihood) for classification\n",
    "    # nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0f054d0-915f-422d-a4ee-6141862765b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.287839  [    0/60000]\n",
      "loss: 0.393056  [ 6400/60000]\n",
      "loss: 0.264242  [12800/60000]\n",
      "loss: 0.330621  [19200/60000]\n",
      "loss: 0.210834  [25600/60000]\n",
      "loss: 0.313641  [32000/60000]\n",
      "loss: 0.132506  [38400/60000]\n",
      "loss: 0.327817  [44800/60000]\n",
      "loss: 0.250915  [51200/60000]\n",
      "loss: 0.295730  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.184740 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.129021  [    0/60000]\n",
      "loss: 0.169862  [ 6400/60000]\n",
      "loss: 0.102016  [12800/60000]\n",
      "loss: 0.181213  [19200/60000]\n",
      "loss: 0.141772  [25600/60000]\n",
      "loss: 0.222638  [32000/60000]\n",
      "loss: 0.058147  [38400/60000]\n",
      "loss: 0.234827  [44800/60000]\n",
      "loss: 0.175381  [51200/60000]\n",
      "loss: 0.214001  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.9%, Avg loss: 0.134200 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.080007  [    0/60000]\n",
      "loss: 0.108530  [ 6400/60000]\n",
      "loss: 0.074242  [12800/60000]\n",
      "loss: 0.091893  [19200/60000]\n",
      "loss: 0.106681  [25600/60000]\n",
      "loss: 0.154032  [32000/60000]\n",
      "loss: 0.038339  [38400/60000]\n",
      "loss: 0.181207  [44800/60000]\n",
      "loss: 0.135461  [51200/60000]\n",
      "loss: 0.156229  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.3%, Avg loss: 0.110878 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.055658  [    0/60000]\n",
      "loss: 0.077115  [ 6400/60000]\n",
      "loss: 0.060501  [12800/60000]\n",
      "loss: 0.048558  [19200/60000]\n",
      "loss: 0.076210  [25600/60000]\n",
      "loss: 0.114931  [32000/60000]\n",
      "loss: 0.026077  [38400/60000]\n",
      "loss: 0.136672  [44800/60000]\n",
      "loss: 0.097539  [51200/60000]\n",
      "loss: 0.105730  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.095627 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.038304  [    0/60000]\n",
      "loss: 0.050594  [ 6400/60000]\n",
      "loss: 0.051180  [12800/60000]\n",
      "loss: 0.025159  [19200/60000]\n",
      "loss: 0.059274  [25600/60000]\n",
      "loss: 0.087789  [32000/60000]\n",
      "loss: 0.021757  [38400/60000]\n",
      "loss: 0.099150  [44800/60000]\n",
      "loss: 0.081855  [51200/60000]\n",
      "loss: 0.066711  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.086343 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "815aa5fd-7d67-4a59-85ae-87461a6286e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_oneLayer_AE.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0b5deda-173d-4321-9007-250312d6235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model_oneLayer_AE.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0db74de5-1236-4913-8b50-2ae3231bb5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork_AE(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
