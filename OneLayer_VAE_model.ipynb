{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "542a3f68-f019-41b6-a2c9-1070807ebda9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Implementation of an Autoencoder (VAE)\n",
    "#### based on the VE implementation and https://sannaperzon.medium.com/paper-summary-variational-autoencoders-with-pytorch-implementation-1b4b23b1763a\n",
    "## Simple linear VAE with one layer into hidden (latent) space 748 -> 256 -> 12 and back 12 -> 256 -> 748\n",
    "\n",
    "#### Build for MINST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c170cac6-0e7e-4c2c-98ed-c26c1c7b2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['MKL_NUM_THREADS'] = '1'\n",
    "#os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "#os.environ['OMP_NUM_THREADS'] = '1'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544b516d-aa5f-4481-b7a0-d73f2f9f8657",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(dataset=mnist_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=mnist_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd006b3-bf5d-4077-be2d-789f3cd15016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   ### needed if Nvidia GPU is available and wanted to use the GPU\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39e5f45-c408-44e4-97bb-fdb582f3b34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=784, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (encoder_layer): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hidden_to_mu): Linear(in_features=256, out_features=12, bias=True)\n",
      "  (hidden_to_sigma): Linear(in_features=256, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Building the neural network structure ####\n",
    "\n",
    "dim_encoder_decoder=784\n",
    "dim_hidden=256\n",
    "dim_latent=12\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, dim_encoder_decoder=784, dim_hidden=256, dim_latent=12):\n",
    "        super(VAE , self).__init__()\n",
    "        \n",
    "        self.decoder = nn.Sequential(                             # sequential operation of the following code:\n",
    "            nn.Linear(dim_latent, dim_hidden),                    # decoding laten layer (12 -> 256 nodes)\n",
    "            nn.ReLU(),                                            #\n",
    "            nn.Linear(dim_hidden, dim_encoder_decoder),           # reconstruction of image (256 -> 784)\n",
    "            nn.Sigmoid()                                          # rescale values between ]0,1[ by the sigmoidal function\n",
    "        )\n",
    "        \n",
    "        self.encoder_layer = nn.Sequential(                       # sequential operation of the following code:\n",
    "            nn.Linear(dim_encoder_decoder, dim_hidden),           # decoding laten layer (784 -> 256 nodes)\n",
    "            nn.ReLU(),      \n",
    "        )\n",
    "        \n",
    "        self.hidden_to_mu = nn.Linear(dim_hidden, dim_latent)     # hidden layer in dim_latent for mu\n",
    "        self.hidden_to_sigma = nn.Linear(dim_hidden, dim_latent)  # hidden layer in dim_latent for sigma\n",
    "\n",
    "\n",
    "    def encoder(self, x):\n",
    "        hidden_layer = self.encoder_layer(x)\n",
    "        mu = self.hidden_to_mu(hidden_layer)\n",
    "        sigma = self.hidden_to_sigma(hidden_layer)\n",
    "        return mu , sigma\n",
    "\n",
    "        \n",
    "    def forward(self, x):                                        # \n",
    "        mu, sigma = self.encoder(x)\n",
    "        epsilon = torch.randn_like(sigma)                        #### \"Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1\"\n",
    "        z = mu + sigma*epsilon\n",
    "        decoded = self.decoder(z) \n",
    "        return decoded , mu, sigma , z\n",
    "\n",
    "\n",
    "model = VAE().to(device) #### this is needed for cuda\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c03c64-62cb-4f52-a5a0-4ac0231c598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss() ###   .CrossEntropyLoss() #           ### nn.MSELoss() (Mean Square Error) for regression tasks\n",
    "#loss_fn = nn.BCELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4, )#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a5ad7ec-52cf-4f41-8ed8-2813f0fd0016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:10, 87.60it/s, loss_total=5.42e+5]\n",
      "938it [00:09, 95.62it/s, loss_total=8.31e+4]\n",
      "938it [00:09, 94.30it/s, loss_total=6.25e+4]\n",
      "938it [00:10, 91.10it/s, loss_total=4.28e+4]\n",
      "938it [00:09, 94.00it/s, loss_total=2.64e+4]\n",
      "938it [00:08, 111.68it/s, loss_total=1.46e+4]\n",
      "938it [00:08, 113.43it/s, loss_total=7.41e+3]\n",
      "938it [00:08, 113.59it/s, loss_total=3.67e+3]\n",
      "938it [00:08, 115.90it/s, loss_total=1.88e+3]\n",
      "938it [00:08, 108.27it/s, loss_total=1.09e+3]\n",
      "938it [00:08, 105.72it/s, loss_total=707]\n",
      "938it [00:08, 108.69it/s, loss_total=481] \n",
      "938it [00:08, 116.85it/s, loss_total=346] \n",
      "938it [00:08, 116.10it/s, loss_total=249] \n",
      "938it [00:08, 116.04it/s, loss_total=183] \n",
      "938it [00:08, 113.44it/s, loss_total=139] \n",
      "938it [00:08, 116.81it/s, loss_total=110] \n",
      "938it [00:08, 116.08it/s, loss_total=92.6]\n",
      "938it [00:08, 114.46it/s, loss_total=84.1]\n",
      "938it [00:08, 115.98it/s, loss_total=77.6]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    loop = tqdm(enumerate(train_dataloader))\n",
    "    loss_total = 0\n",
    "    for batch , (X, label) in loop:\n",
    "        X = X.reshape(-1, dim_encoder_decoder).to(device)\n",
    "        pred , mu, sigma , _ = model(X)\n",
    "        re_loss = loss_fn(pred, X)\n",
    "        kl_div = - torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n",
    "        loss = re_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_total += loss.item()\n",
    "        loop.set_postfix(loss_total=loss_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3918288-c4d2-4f54-bf73-43371fbe53a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5deda-173d-4321-9007-250312d6235e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db74de5-1236-4913-8b50-2ae3231bb5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce76be-1183-4567-b67a-d657f01f10ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
